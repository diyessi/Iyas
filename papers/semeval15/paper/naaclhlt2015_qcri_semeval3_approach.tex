\section{Features}
\label{sec:approach}

In this section, we describe the different features we considered including 
similarity measures (Section~\ref{sub:sim}), the context in which a comment 
appears (Section~\ref{ssub:context}), and the occurrence of certain vocabulary 
and phrase triggers (Sections~\ref{ssub:ngrams} and~\ref{ssub:heuristics}). How 
and where we apply them is discussed in Section~\ref{sec:experiments}.
Note that while our general approach is based on supervised machine learning,
some of our contrastive submissions are rule-based.

% \subsection{Arabic task}
% \label{sub:app_arabic}
% 
% \begin{description}
%  \item[Lexical similarity]  Massimo, Hamdy's overlap (same as in A) 
% \end{description}

\subsection{Similarity Measures}
\label{sub:sim}

The similarity features measure the similarity $sim(q,c)$
between the question and a target comment,
assuming that high similarity signals a \good answer.
We consider three kinds of similarity measures, which we describe below.

\subsubsection{Lexical Similarity}

We compute the similarity between word $n$-gram representations 
($n=[1,\ldots,4]$) of $q$ and $c$,
using the following lexical similarity measures
(after stopword removal):
greedy string tiling~\cite{Wise:1996},
longest common subsequences~\cite{Allison:1986},
Jaccard coefficient~\cite{Jaccard:1901},
word containment~\cite{Lyon:2001}, and cosine similarity.
We further compute cosine on lemmata and POS tags, either including stopwords or not.

% \blue{Wei}
% We computed three features on the intersection of the tokens in both $q$ and 
% $c$ by considering three term weighting schemes:
We also use similarity measures, which weigh the terms
using the following three formul\ae: 
%
\begin{align}
sim(q,c) = &
	\sum_{t\in q\cap c} & idf(t) \enspace		\\
sim(q,c) = & \sum_{t\in q\cap c} & log(idf(t)) \label{idfvar1}\\
sim(q,c) = & \sum_{t\in q\cap c} & log\left(1 + \frac{|C|}{tf(t)}\right) \label{idfvar2}
\end{align}
%\begin{eqnarray}
% sim(q, c)=\sum_{t\in q\cap c} & idf(t) \enspace,		\\
% sim(q, c)=\sum_{t\in q\cap c} & log(idf(t)) \enspace, \enspace \mathrm{and} 
%\label{idfvar1}\\
% sim(q, c)=\sum_{t\in q\cap c} & log\left(1 + \frac{|C|}{tf(t)}\right) \enspace 
%,
%\label{idfvar2}
%\end{eqnarray}
% 
where $idf(t)$ is the inverse document frequency~\cite{Jones:1972} of term $t$ 
in the entire Qatar Living dataset, $C$ is the number of comments in this collection, 
and $tf(t)$ is the term frequency of the term in the comment. 
Equations~\ref{idfvar1} and~\ref{idfvar2} are variations of idf;
cf. Nallapati~\shortcite{Nallapati:2004}.

For subtask B, we further considered 
the cosine similarity between the $tf$-$idf$-weighted intersection of the words in $q$ and $c$.
% . In this case, the weight associated to each word is its tf-idf.

% where $idf(t)$ represents the inverse document frequency of term $t$ in the 
% entire Qatar Living dataset%
% \footnote{All the \textit{idf} values were computed on the Qatar Living dataset, 
% distributed by the task organizers~\cite{Marquez-EtAl:2015:SemEval}: 
% \url{http://alt.qcri.org/semeval2015/task3/}.},
% $|C|$ represents the amount of comments in the entire collection, and $tf(t)$ 
% represents the term frequency of the term in the comment. These are variations 
% of the \textit{idf} concept by \blue{Salton (1986)} and 
% \blue{Nallapati (2004)}.\footnote{\blue{Wei, please add the proper references}}

\subsubsection{Syntactic Similarity}
\label{sub:syntactic}

We further use a partial tree kernel \cite{Moschitti:2006}
to calculate the similarity between the question and the comment
based on their corresponding shallow syntactic trees.
These trees have word lemmata as leaves,
then there is a POS tag node parent for each lemma leaf,
and POS tag nodes are in turn grouped under shallow parsing chunks,
which are linked to a root sentence node;
finally, all root sentence nodes
are linked to a super root for all sentences in the question/comment.


\subsubsection{Semantic Similarity}
\label{sub:semantic}

We apply three approaches to build word-embedding vector representations,
using
\Ni latent semantic analysis~\cite{croce-previtali:2010:GEMS}, 
trained on the Qatar Living corpus with a word co-occurrence window of size 
$\pm3$ and producing a vector of 250 dimensions with SVD
(we produced a vector for each noun in the vocabulary); 
\Nii GloVe~\cite{Pennington:2014}, using a model pre-trained on \textit{Common 
Crawl (42B tokens)}, with 300 dimensions; and 
\Niii COMPOSES~\cite{Baroni:2014}, using previously-estimated predict vectors of 
400 dimensions.%
\footnote{They are available at \url{http://nlp.stanford.edu/projects/glove/} 
and \url{http://clic.cimec.unitn.it/composes/semantic-vectors.html}}
% ; last visit: Jan 6th, 2015.}
We represent both $q$ and $c$ as 
a sum of the vectors corresponding to the words within them (neglecting the 
subject of $c$). We compute the cosine similarity to estimate $sim(q,c)$. 

We also experimented with \textit{word2vec}~\cite{Mikolov:2013} vectors 
pre-trained with both \emph{cbow} and \emph{skipgram} on news data, 
and also with both word2vec and GloVe vectors trained on Qatar Living data,
but we discarded them as they did not help us on top of all other features we had.

\subsection{Context}
\label{ssub:context}

Comments are organized sequentially according to the time line of the comment thread.
Whether a question includes further comments by the person who asked the original question
or just several comments by the same user,
or whether it belongs to a category in which a given kind of answer is expected,
are all important factors.
Therefore, we consider a set of features that try to describe
a comment in the context of the entire comment thread.

We have boolean context features that explore the following situations:

\begin{itemize}\setlength\itemsep{-0.4em}
\item $c$ is written by $u_q$ (i.e., the same user behind $q$),
\item \label{enu:context_ack} 
  $c$ is written by $u_q$ and contains an acknowledgment (\eg
  \textit{thank*}, \textit{appreciat*}),
\item \label{enu:context_quest}
  $c$ is written by $u_q$ and includes further question(s), and 
\item $c$ is written by $u_q$ and includes no acknowledgments nor further 
questions.
\end{itemize}
% 
We further have numerical features exploring whether comment $c$ appears in the proximity of a 
comment by $u_q$; the assumption is that an acknowledgment or further 
questions by $u_q$ could signal a bad answer:
%Features investigating the following occurrences have been developed:

\begin{itemize}\setlength\itemsep{-0.4em}
\item among the comments following $c$ there is one by $u_q$ containing 
an acknowledgment,
\item among the comments following $c$ there is one by $u_q$ not 
containing an acknowledgment,
\item among the comments following $c$ there is one by $u_q$ containing a 
question, and
\item among the comments preceding $c$ there is one by $u_q$ containing a 
question.
\end{itemize}

The numerical value of these last four features is determined by the distance $k$,
in number of comments, between $c$ and the closest comment by $u_q$
($k=\infty$ if no comments by $u_q$ exist):
%Our function to represent the relationship between a comment 
%$c_{t-k}$ in time $t-k$ and $c_{q,t}$, given that $t$ is the 
%time of the comment by $u_q$ is as follows:
% 
\begin{equation}
 f(c)=\max \left(0,\enspace 1.1-(k \cdot 0.1) \right)
\end{equation}
%
%where $k$ is the distance between $c_{t-k}$ and $c_{q,t}$ in the past.

We also tried to model potential dialogues by identifying interlacing comments between two users.
Our dialogue features rely on identifying conversation chains between two users:
\begin{align*}
u_i \rightarrow \ldots \rightarrow 
u_j \rightarrow \ldots \rightarrow
u_i \rightarrow \ldots \rightarrow
[u_j]
\end{align*}
% 
%where $u_i$ and $u_j$ are the authors of $c_i$ and $c_j$. 
Note that comments by other users can appear in between the nodes of this ``pseudo-conversation'' chain.
We consider three features:
whether a comment is at the beginning, in the middle, or at the end of such a chain.
We have copies of these three features for the special case when $u_q=u_j$. 

We are also interested in modeling whether a user $u_i$ has been particularly 
active in a question thread. Thus, we add one boolean feature:
whether $u_i$ wrote more than one comment in the current thread.

Three more features 
identify the first, the middle and the last comments by $u_i$.
One extra feature counts the total number of comments written by $u_i$.
Moreover, we empirically observed 
that the likelihood of a comment being \good decreases with its position in the thread.
Therefore, we also include another real-valued feature: 
$\max(20, i)/20$, where $i$ represents the position of the comment in the thread.

Finally, Qatar Living includes twenty-six different categories in which one
could request information and advice. Some of them tend to include more 
open-ended questions and even invite discussion on ambiguous topics, \eg
\textit{Socialising}, \textit{Life in Qatar}, \textit{Qatari Culture}. Some other require more 
precise answers and allow for less discussion, \eg \textit{Visas and Permits}. 
Therefore, we include one boolean feature per category to consider this 
information. 
 
\subsection{Word $n$-Grams}
\label{ssub:ngrams}
%\blue{Reapproached subsection. Original commented}

Our features include $n$-grams, independently obtained from both the question and the comment:
$[1,2]$-grams for Arabic, and stopworded $[1,2,3]$-grams for English. That is, 
each $n$-gram appearing in the texts becomes a member of the feature vector. 
The value for such features is tf-idf, with idf computed on the entire Qatar Living dataset.

Our aim is to capture the words that are associated with questions and comments 
in the different classes. We assume that objective and clear questions would tend to produce objective and \good comments. On the other hand, subjective or badly formulated 
questions would call for \bad comments or discussion, i.e., dialogues, among 
the users. This can be reflected by the vocabulary used, regardless of the topic of the formulated question. This is also true for comments: the occurrence of particular words could make a comment more likely to be \good or \bad, regardless of what question was asked.
 
%We assume that a properly produced question should allow for the creation of 
%\good comments. That is, objective and clear questions would tend to produce 
%objective and \good comments. On the other side, subjective or badly formulated 
%questions would call for \bad comments or even discussion (\ie dialogues) among 
%the users. When talking about comments, they could also include specific 
%indicators that trigger a \good or a \bad class, regardless of the specific 
%question they intends to reply to. Our aim is to capture those words which 
%are associated with questions and comments in the different classes. 
%
%Our features include $n$-grams, independently obtained from the question and the
%comments ---$[1,2]$-grams for Arabic and stopworded $[1,2,3]$-grams for English. 
%The weights are based on tf-idf on the entire Qatar Living dataset. 

\subsection{Heuristics}
\label{ssub:heuristics}

Exploring the training data, we noticed that many \good comments suggested visiting a Web 
site or contained an email address. Therefore, we included two boolean 
features 
to verify the presence of URLs or emails in $c$. Another feature captures the 
length of $c$, as longer (\good) comments usually contain detailed information 
to answer a question. 



% \blue{TODO complete these}
% 
% \begin{itemize}
%  \item A boolean feature, whether $c$ contains a URL or electronic mail. 
%  \item the length of $c_i$ in characters, as we empirically observed that long 
%   comments tend to be \good.
% \blue{simone}
% \end{itemize}
% 
% 
% \blue{Hamdy's contrastive}
% Our contrastive submission \blue{x} is a rule-based system. A comment is 
% labeled as \good if starts with one of a set of imperative verbs, including 
% \textit{try}, \textit{view}, \textit{contact}, \textit{check}%
% \footnote{
% % yes list: {"yes", "yep", "yup", "yap", "yeah", "yea", "ya", "yess", 
% "yeh", "sure"} --> both yes and good
% 2- no list: {"no", "noo", "nooo", "nop", "nope"}
%  --> both no and good
% % 3- thanks/dialogue list: {"thank", "thx", "thanks", "thanx", "thnk", "tnx", 
% "thnak", "sorri", "welcom", "wow"} --> dialogues
% 4. generic answers list: {"check", "try", "go", "call", "contact", "follow", 
% "go", "take", "talk", "use", "visit", "watch"} --> good


%HAMDYS
%ENGLISH:
% 11. if score == highest score                                          -> Good
% 12. if score >= 0.5 of the second highest score             -> Good
% 13. if score == 0                                                              
% -> Bad  Otherwise                                                         
%           -> Potential
% [12:35:11 PM] Hamdy Mubarak: Porter Stemmer problems:
% - it gives incorrect stem when word starts with capital letter (at the beginning 
% of sentence). ex: Lady, Ladies and lady will give different stems
% - stem is not always correct when a named entity written in small letters, ex: 
% Los angeles.
% - it doesn't have a built-in spell checker to handle spelling mistakes
% 
% I used a list of ~30,000 words and their stems (lookup table):
% http://snowball.tartarus.org/algorithms/english/diffs.txt
% 
% The training data is stemmed using the word list, and stopwords are marked by 
% revising the top 3,000 words (freq >= 15)
% 
% 
% \blue{complete it or cite the source for affirmative words; the same 
% for the rest}}, ..” and includes a URL or phone number. A comment is labeled 
% as \dial if it starts with \textit{thanks}, \textit{thx}, \textit{thanx},..” 
% or it has been written by the same person that asked the question.%
% \footnote{\blue{I am tempted to include a simple table with all the 
% vocabularies in these rules. TODO check these vocabularies}}
% 


% \bsegin{description}
%  \item[Lexical similarity] 
%  \item[Syntactic similarity] Massimo's PTK
%  \item[Semantic similarity] (Preslav, Simone's LSA)
%  \item[Context-based] (Simone)
%  \item[Heuristics] Hamdy's
% \end{description}

\subsection{Polarity}
\label{sub:polarity}

These features, which we used for subtask B only,
try to determine whether a comment is positive or negative,
which could be associated with \yes or \no answers.
The polarity of a comment $c$ is 
%a real value computed as follows:
\begin{equation}
pol(c) = \sum_{w\in c} pol(w) 
\end{equation}
%
where $pol(w)$ is the polarity of word $w$ in the NRC Hashtag Sentiment 
Lexicon v0.1~\cite{MohammadKZ2013}.
%\footnote{\url{http://saifmohammad.com/WebPages/lexicons.html}.}
% ; last visit: Jan 18, 2015.}
%The polarities in this lexicon 
%are not ranged, and the ones 
%that are close to zero are associated with neutral words.
%Therefore, 
We disregarded $pol(w)$ if its absolute value was less than 1.

We further use boolean features that check the existence of some keywords in the comment.
Their values are set to true if $c$ contains words like
\Ni \textit{yes}, \textit{can}, \textit{sure}, \textit{wish}, \textit{would}, or
\Nii \textit{no}, \textit{not}, \textit{neither}.
% ; or 
% \Niii  a \textit{URL}.

% \blue{together with others intending to model \yes, \no, and \unsure answers.}

\subsection{User Profile}
\label{sub:profile}

With this set of features, we aim to model the behavior of the different 
participants in previous queries. Given comment $c$ by user $u$, we consider 
the number of \good, \bad, \pot, and \dial comments $u$ has produced 
before.\footnote{About 72\% of the comments in the test set were written by users who 
had been seen in the training/development set.}
%NUMBERS BY IMAN ON MARCH 17th:
%There are 5,881 users in the training set (train + dev combined). 4,954 
%users write comments and 2,170 write questions. For the test set there are 1,047 
%users. 912 users write comments and 275 ask questions. The overlap between 
%those who write comments is 514 users, and for those who write questions the 
%overlap is 76.
%The total overlap of all users in both train and test is 579 users.
We also consider the average word length of \good, \bad, \pot, and \dial 
comments. These features are computed both considering all questions and 
taking into account only those from the target category.
\footnote{In Section~\ref{sec:discussionb}, we will observe that computing 
these category-level features was not a good idea.}


% \blue{these two features were under consideration already:}
% length of the comment, and the inverse rank of the comment in the list of all 
% comments for a question.


% \begin{description}
%  \item[Lexical similarity]  Iman
%  \item[Heuristics] Iman's ``lexical features''
%  \item[Sentiment] Iman
%  \item[Context] Iman's user profiles
% \end{description}

