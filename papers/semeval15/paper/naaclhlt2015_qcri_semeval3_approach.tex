\section{Features Description}
\label{sec:approach}

We built most of our approaches on top of supervised machine learning, whereas 
a few contrastive submissions were based on rule-based approaches. In this 
section, we describe all the different features we considered including 
similarity measures (Section~\ref{sub:sim}), the context in which a comment 
appears (Section~\ref{ssub:context}), and the occurrence of certain vocabulary 
and phrase triggers (Sections~\ref{ssub:ngrams} and~\ref{ssub:heuristics}). How 
and where they are applied is discussed in Section~\ref{sec:experiments}.

% \subsection{Arabic task}
% \label{sub:app_arabic}
% 
% \begin{description}
%  \item[Lexical similarity]  Massimo, Hamdy's overlap (same as in A) 
% \end{description}

\subsection{Similarity Measures}
\label{sub:sim}

Our assumption is that the higher the similarity  $sim(q,c)$, the higher the 
likelihood that $c$ is a \good answer. We consider three different types.

\subsubsection{Lexical Similarity}

After stopwording, we compute $sim(q, c)$ for word $n$-gram representations 
($n=[1,\ldots,4]$) of $q$ and $c$, and different $sim$ functions: greedy 
string tiling~\cite{Wise:1996}, longest common 
subsequences~\cite{Allison:1986}, Jaccard coefficient~\cite{Jaccard:1901}, word 
containment~\cite{Lyon:2001}, and cosine similarity (cosine is also computed on 
lemmas and POS tags, either including stopwords or not).

% \blue{Wei}
% We computed three features on the intersection of the tokens in both $q$ and 
% $c$ by considering three term weighting schemes:
Three other similarity measures are computed, weighting the terms by means of 
three 
formul\ae: 
%
\begin{align}
sim(q,c) = &
	\sum_{t\in q\cap c} & idf(t) \enspace,		\\
 	& \sum_{t\in q\cap c} & log(idf(t)) \enspace, \enspace \mathrm{and} 
\label{idfvar1}\\
 	& \sum_{t\in q\cap c} & log\left(1 + \frac{|C|}{tf(t)}\right) \enspace , \label{idfvar2}
\end{align}
%\begin{eqnarray}
% sim(q, c)=\sum_{t\in q\cap c} & idf(t) \enspace,		\\
% sim(q, c)=\sum_{t\in q\cap c} & log(idf(t)) \enspace, \enspace \mathrm{and} 
%\label{idfvar1}\\
% sim(q, c)=\sum_{t\in q\cap c} & log\left(1 + \frac{|C|}{tf(t)}\right) \enspace 
%,
%\label{idfvar2}
%\end{eqnarray}
% 
where $idf(t)$ is the inverse document frequency~\cite{Jones:1972} of term $t$ 
in the entire Qatar Living dataset, $C$ is the amount of comments in this collection, 
and $tf(t)$ is the term frequency of the term in the comment. 
Equations~\ref{idfvar1} and~\ref{idfvar2} are variations of the IDF concept by 
Nallapati~\shortcite{Nallapati:2004}.

We considered yet another similarity variation (only for task B): the cosine 
similarity between the $tf$-$idf$-weighted vocabulary intersection of $q$ and 
$c$.
% . In this case, the weight associated to each word is its tf-idf.

% where $idf(t)$ represents the inverse document frequency of term $t$ in the 
% entire Qatar Living dataset%
% \footnote{All the \textit{idf} values were computed on the Qatar Living dataset, 
% distributed by the task organizers~\cite{Marquez-EtAl:2015:SemEval}: 
% \url{http://alt.qcri.org/semeval2015/task3/}.},
% $|C|$ represents the amount of comments in the entire collection, and $tf(t)$ 
% represents the term frequency of the term in the comment. These are variations 
% of the \textit{idf} concept by \blue{Salton (1986)} and 
% \blue{Nallapati (2004)}.\footnote{\blue{Wei, please add the proper references}}

\subsubsection{Syntactic Similarity}
\label{sub:syntactic}

Partial tree kernel (PTK) similarity between the question and the comment 
syntactic trees according to~\cite{Moschitti:2006}. The similarity is computed 
between shallow tree  representations of $q$ and $c$. Such trees have lemmas as 
leaves, each leaf has a parent node representing a part-of-speech tag, and 
part-of-speech nodes are grouped by chunks at the top level.

\subsubsection{Semantic Similarity}
\label{sub:semantic}

We apply three approaches to build  word-embedding vector representations:
\Ni an instance of latent semantic analysis~\cite{croce-previtali:2010:GEMS}, 
trained on the Qatar Living corpus applying a co-occurrence window of size 
$\pm3$ and producing a vector of 250 dimensions, after SVD reduction (we 
included an instance on the entire vocabulary and nouns only); 
\Nii GloVe~\cite{Pennington:2014}, using a model pre-trained on \textit{Common 
Crawl (42B tokens)}, with 300 dimensions; and 
\Niii COMPOSES~\cite{Baroni:2014}, using previously-estimated predict vectors of 
400 dimensions.%
\footnote{They are available at \url{http://nlp.stanford.edu/projects/glove/} 
and \url{http://clic.cimec.unitn.it/composes/semantic-vectors.html}}
% ; last visit: Jan 6th, 2015.}
We also experimented with \textit{word2vec}~\cite{Mikolov:2013} vectors 
pre-trained (both with cbow and skipgram) and both word2vec and GloVe with 
vectors trained on Qatar Living data, but we discarded them, as they did not 
contribute positively to our approach. We represent both $q$ and $c$ as 
a sum of the vectors corresponding to the words within them (neglecting the 
subject of $c$). We compute the cosine similarity to estimate $sim(q,c)$. 

\subsection{Context}
\label{ssub:context}

Comments are organized sequentially according to the time line of the comment 
thread. Whether a question includes further comments by $u_q$ (some of them 
acknowledging), more than one comment from the same user, or whether $q$ belongs 
to a category in which a given kind of answer is expected, are important 
factors. Therefore, we consider a set of features that try to describe a 
comment in its context.   

A first subset of context features are boolean indicators exploring the 
following situations:

\begin{itemize}\setlength\itemsep{-0.4em}
\item $c$ is written by $u_q$ (\ie the same user behind $q$),
\item \label{enu:context_ack} 
  $c$ is written by $u_q$ and contains and acknowledgment (\eg   
  \textit{thank*}, \textit{appreciat*}),
\item \label{enu:context_quest}
  $c$ is written by $u_q$ and includes further questions, and 
\item $c$ is written by $u_q$ and includes no acknowledgments nor further 
questions.
\end{itemize}
% 
A second subset explores whether comment $c$ appears in the proximity of a 
comment by $u_q$. The assumption is that acknowledgment or further 
questions by 
$u_q$ could be a relevant factor when classifying $c$. Features investigating 
the following occurrences have been developed:

\begin{itemize}\setlength\itemsep{-0.4em}
\item among the comments following $c$ there is one by $u_q$ containing 
an acknowledgment,
\item among the comments following $c$ there is one by $u_q$ not 
containing an acknowledgment,
\item among the comments following $c$ there is one by $u_q$ containing a 
question, and
\item among the comments preceding $c$ there is one by $u_q$ containing a 
question.
\end{itemize}

The value of these features is scaled according to the distance $k$, in number 
of comments, between $c$ and the comment by $u_q$ ($k=\infty$ if no 
comments from $u_q$ exist):
%Our function to represent the relationship between a comment 
%$c_{t-k}$ in time $t-k$ and $c_{q,t}$, given that $t$ is the 
%time of the comment by $u_q$ is as follows:
% 
\begin{equation}
 f(c)=\max \left(0\enspace,\enspace 1.1-(k \cdot 0.1) \right)
\end{equation}
%
%where $k$ is the distance between $c_{t-k}$ and $c_{q,t}$ in the past.

We also tried to model potential dialogues by identifying interlacing comments 
between two users. Our dialogue features rely on identifying comments from
a sequence of users 
\begin{align*}
u_i \rightarrow \ldots \rightarrow 
u_j \rightarrow \ldots \rightarrow
u_i \rightarrow \ldots \rightarrow
[u_j] \enspace ,
\end{align*}
% 
%where $u_i$ and $u_j$ are the authors of $c_i$ and $c_j$. 
Note that comments by other 
users can appear in between this ``pseudo-conversation''. Three features are 
considered, whether a comment is at the beginning, middle, or ending position of 
the pseudo-dialogue. We consider three more features for those cases in which 
$u_q=u_j$. 

We are also interested in modeling whether a user $u_i$ has been particularly 
active in a question. As a result, we consider one boolean feature: whether 
$u_i$ wrote more than one comment in the current stream. Three more features 
identify the first, middle and last comments by $u_i$. One extra feature counts 
the total number of comments written by $u_i$. Moreover we empirically observed 
that the likelihood for a comment to be \good decreases the farther it appears 
from the question. Therefore, we consider one more real-valued feature: 
$\max(20, i)/20$, where $i$ represents the position of the comment in the 
stream.

Finally, Qatar Living includes twenty-six different categories in which a person 
could request information and advice. Some of them tend to include more 
open questions and even invite discussions on ambiguous topics (\eg
\textit{life in Qatar}, \textit{Qatari culture}). Some others require more 
precise answers and allow for less discussion (\eg \textit{visas and permits}). 
Therefore, we include one boolean feature per category to consider this 
information. 
 
\subsection{Word $n$-Grams}
\label{ssub:ngrams}
\blue{Reapproached subsection. Original commented}

Our features include $n$-grams, independently obtained from both $q$ and $c$ 
---$[1,2]$-grams for Arabic and stopworded $[1,2,3]$-grams for English. That is, 
each $n$-gram appearing in the texts becomes a member of the feature vector. 
The value for such features is given by the tf-idf, computed on the entire Qatar 
Living dataset.

Our aim is to capture those words which are associated with questions and comments 
in the different classes. We assume that objective and clear questions would tend to produce objective and \good comments. On the other side, subjective or badly formulated 
questions would call for \bad comments or discussion (\ie dialogues) among 
the users. This can be reflected by the vocabulary used, regardless of the topic of the formulated question. This is also true for comments: the occurrence of particular (sets of) words could make a comment either \good or \bad.
 
%We assume that a properly produced question should allow for the creation of 
%\good comments. That is, objective and clear questions would tend to produce 
%objective and \good comments. On the other side, subjective or badly formulated 
%questions would call for \bad comments or even discussion (\ie dialogues) among 
%the users. When talking about comments, they could also include specific 
%indicators that trigger a \good or a \bad class, regardless of the specific 
%question they intends to reply to. Our aim is to capture those words which 
%are associated with questions and comments in the different classes. 
%
%Our features include $n$-grams, independently obtained from the question and the
%comments ---$[1,2]$-grams for Arabic and stopworded $[1,2,3]$-grams for English. 
%The weights are based on tf-idf on the entire Qatar Living dataset. 

\subsection{Heuristics}
\label{ssub:heuristics}

Exploring the data, we noticed that many \good comments suggested visiting a Web 
site or contained an email address. Therefore, we included two boolean 
features 
to verify the presence of URLs or emails in $c$. Another feature captures the 
length of $c$, as longer (\good) comments usually contain detailed information 
to answer a question. 



% \blue{TODO complete these}
% 
% \begin{itemize}
%  \item A boolean feature, whether $c$ contains a URL or electronic mail. 
%  \item the length of $c_i$ in characters, as we empirically observed that long 
%   comments tend to be \good.
% \blue{simone}
% \end{itemize}
% 
% 
% \blue{Hamdy's contrastive}
% Our contrastive submission \blue{x} is a rule-based system. A comment is 
% labeled as \good if starts with one of a set of imperative verbs, including 
% \textit{try}, \textit{view}, \textit{contact}, \textit{check}%
% \footnote{
% % yes list: {"yes", "yep", "yup", "yap", "yeah", "yea", "ya", "yess", 
% "yeh", "sure"} --> both yes and good
% 2- no list: {"no", "noo", "nooo", "nop", "nope"}
%  --> both no and good
% % 3- thanks/dialogue list: {"thank", "thx", "thanks", "thanx", "thnk", "tnx", 
% "thnak", "sorri", "welcom", "wow"} --> dialogues
% 4. generic answers list: {"check", "try", "go", "call", "contact", "follow", 
% "go", "take", "talk", "use", "visit", "watch"} --> good


%HAMDYS
%ENGLISH:
% 11. if score == highest score                                          -> Good
% 12. if score >= 0.5 of the second highest score             -> Good
% 13. if score == 0                                                              
% -> Bad  Otherwise                                                         
%           -> Potential
% [12:35:11 PM] Hamdy Mubarak: Porter Stemmer problems:
% - it gives incorrect stem when word starts with capital letter (at the beginning 
% of sentence). ex: Lady, Ladies and lady will give different stems
% - stem is not always correct when a named entity written in small letters, ex: 
% Los angeles.
% - it doesn't have a built-in spell checker to handle spelling mistakes
% 
% I used a list of ~30,000 words and their stems (lookup table):
% http://snowball.tartarus.org/algorithms/english/diffs.txt
% 
% The training data is stemmed using the word list, and stopwords are marked by 
% revising the top 3,000 words (freq >= 15)
% 
% 
% \blue{complete it or cite the source for affirmative words; the same 
% for the rest}}, ..” and includes a URL or phone number. A comment is labeled 
% as \dial if it starts with \textit{thanks}, \textit{thx}, \textit{thanx},..” 
% or it has been written by the same person that asked the question.%
% \footnote{\blue{I am tempted to include a simple table with all the 
% vocabularies in these rules. TODO check these vocabularies}}
% 


% \bsegin{description}
%  \item[Lexical similarity] 
%  \item[Syntactic similarity] Massimo's PTK
%  \item[Semantic similarity] (Preslav, Simone's LSA)
%  \item[Context-based] (Simone)
%  \item[Heuristics] Hamdy's
% \end{description}

\subsection{Polarity}
\label{sub:polarity}

These features, used for task B only, intend to determine whether a comment is 
positive or negative, which could be associated to \yes or \no answers. The
polarity of $c$ is \blue{a real value computed} as:
\begin{equation}
pol(c) = \sum_{w\in c} pol(w) 
\end{equation}
%
where $pol(w)$ is the polarity of word $w$ in the NRC Hashtag Sentiment 
Lexicon v0.1~\cite{MohammadKZ2013}.%
\footnote{\url{%
http://www.umiacs.umd.edu/~saif/WebPages/Abstracts/NRC-SentimentAnalysis.htm}.}
% ; last visit: Jan 18, 2015.}
\blue{The polarities in this lexicon are not ranged and the ones closest to zero are associated to nearly neutral words. Therefore, we disregard $pol(w)$ if it is in the range $(-1,1)$.}

We consider other boolean features on the existence of some keywords in the 
comment. Features are set to true if $c$ contains
\Ni \textit{yes}, \textit{can}, \textit{sure}, \textit{wish}, \textit{would} or
\Nii \textit{no}, \textit{not}, \textit{neither}.
% ; or 
% \Niii  a \textit{URL}.

% \blue{together with others intending to model \yes, \no, and \unsure answers.}

\subsection{User's Profile}
\label{sub:profile}

With this set of features, we aim at modeling the behavior of the different 
participants in previous queries. Given comment $c$ by user $u$, we consider 
the number of \good, \bad, \pot, and \dial comments the user has produced 
before. 
\footnote{\blue{Circa 72\% of the comments in the test set were written by users who 
had been seen in the training/development set.}}
%NUMBERS BY IMAN ON MARCH 17th:
%There are 5,881 users in the training set (train + dev combined). 4,954 
%users write comments and 2,170 write questions. For the test set there are 1,047 
%users. 912 users write comments and 275 ask questions. The overlap between 
%those who write comments is 514 users, and for those who write questions the 
%overlap is 76.
%The total overlap of all users in both train and test is 579 users.
We also consider the average word length of \good, \bad, \pot, and \dial 
comments. These features are computed both considering all the questions and 
only those from the same category as the current one.%
\footnote{In Section~\ref{sec:discussionb}, we will observe that computing 
these category-level statistics was not a good idea.}


% \blue{these two features were under consideration already:}
% length of the comment, and the inverse rank of the comment in the list of all 
% comments for a question.


% \begin{description}
%  \item[Lexical similarity]  Iman
%  \item[Heuristics] Iman's ``lexical features''
%  \item[Sentiment] Iman
%  \item[Context] Iman's user profiles
% \end{description}

