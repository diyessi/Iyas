\section{Introduction}
\label{sec:intro}


SemEval-2015 Task 3 ``Answer Selection in Community Question Answering''
challenged the participants to automatically predict the 
appropriateness of the answers in a community question answering 
setting \cite{Marquez-EtAl:2015:SemEval}.
Given a question $q\in Q$ asked by user $u_q$ and a set of comments $C$,
the main task was to determine whether a comment $c\in C$ offered a suitable 
answer to $q$ or not. 

In the case of Arabic, the questions were extracted from \textit{Fatwa}, a 
community question answering website about Islam.%
\footnote{\url{http://fatwa.islamweb.net}} 
Each question includes five comments, provided by scholars on 
the topic, each of which has to be automatically labeled as 
\Ni \dir: a direct answer to the question;
\Nii \rel: not a direct answer to the question but with information related to 
the topic; and 
\Niii \irel: an answer to another question, not related to the topic. 
This is subtask A, Arabic.

In the case of English, the dataset was extracted from \textit{Qatar Living}, 
a forum for people to pose questions on multiple aspects of daily life in 
Qatar.%
\footnote{\url{http://www.qatarliving.com/forum}}
Unlike \textit{Fatwa}, the questions and comments in this dataset come from 
regular users, making them significantly more varied, informal, open, and noisy. 
In this case, the input to the system consists of a question and a variable 
number of comments, each of which is to be labeled as 
\Ni \good: the comment is definitively relevant; 
\Nii \pot: the comment is potentially useful; and 
\Niii \bad: the comment is irrelevant (\eg it is part of a dialogue, unrelated 
to the topic, or it is written in a language other than English). 
This is subtask A, English.

Additionally, a subset of the questions required a \yes/\no answer,
% , which means the expected answer to the question is precisely either \yes or 
% \no. 
and there was another subtask for them,
which asked to determine whether the overall answer to the question,
according to the evidence provided by the comments, is 
\Ni \yes, 
\Nii \no, or 
\Niii \unsure.
% when there is no evidence to make a decision. 
This is subtask B, English.

Details about the subtasks and the experimental settings
can be found in \cite{Marquez-EtAl:2015:SemEval}.

Below we describe the supervised learning approach of QCRI\@,
which considers different kinds of features: lexical, syntactic and semantic similarities; 
the context in which a comment appears; $n$-grams occurrence; and some heuristics.
We ranked first in the Arabic, and third in the two English subtasks.
%Our team ranked 1st out of four teams in the 
%Arabic subtask, 3rd out of twelve in English subtask~A, and 3rd out of eight in English subtask~B. 

The rest of the paper is organized as follows:
Section~\ref{sec:approach} describes the features used,
Section~\ref{sec:experiments} discusses our models and our official results, and
Section~\ref{sec:discussion} presents post-competition experiments and offers some final remarks.
