\section{Post-Submission Experiments}
\label{sec:discussion}

We carried out post-submission experiments in order to understand how 
different feature families contributed to the performance of our classifiers;
the results are shown in Table~\ref{tab:aftertask}.
% The results on the corresponding test sets are reported in 
%Table~\ref{tab:aftertask} reports the results on the different test sets.
We also managed to improve our performance for all three subtasks.


\subsection{Arabic} \label{sec:discussionArabic}

We ran experiments with the same framework as in our primary submission by 
considering the subsets of features in isolation (\textit{only}) or all features 
except for a subset (\textit{without}). The $n$-gram features together with 
our cont$_1$ submission (recall that we also use cont$_1$ as a feature in our
primary submission) allow for a slightly better performance than our 
---already winning--- primary submission (F$_1=78.69$, compared to 
F$_1=78.55$). The cont$_1$ feature turns out to be the most important one,
and, as it already contains similarity, combining it with other similarity features
does not yield any further improvements.
%When combined with the similarity features, no improvement
%is observed as cont$_1$ is based already upon similarity computations.
%It is worth noting that cont$_1$ 
%includes already similarity computations.

\begin{table}[t]	
\footnotesize
\centering
% \begin{tabular}{|l|c@{\hskip 0.3cm}c@{\hskip 0.3cm}c@{\hskip 0.3cm}c|}
\begin{tabular}{|l|cccc|}
% \hline
%   \multicolumn{5}{c}{Subtask A} \\
\hline  
\bf ar (only)	& \texttt{DIR} & \texttt{IRREL} & \texttt{REL} & 
\texttt{F$_1$} 
\\\hline
 $n$-grams	& 30.40	& 41.07	& 72.27	& 47.91	\\
 cont$_1$	& 74.89	& 63.68	& 91.23	& 76.60	\\
 similarities	& 61.83	& 25.63	& 82.55	& 56.67	\\
%  \bf ar (only)	& \texttt{DIR} & \texttt{REL} & \texttt{IRREL} & \texttt{F$_1$} 
% \\\hline
%  $n$-grams	& 30.40	& 72.27	& 41.07	& 47.91	\\
%  cont$_1$	& 74.89	& 91.23	& 63.68	& 76.60	\\
%  similarities	& 61.83	& 82.55	& 25.63	& 56.67	\\ 
  \hline
  \hline  
 \bf ar (without)& \texttt{DIR} & \texttt{REL} & \texttt{IRREL} &\texttt{F$_1$} 
\\\hline
 $n$-grams	& 75.51	& 91.31	& 63.85	& 76.89	\\
 cont$_1$	& 69.50	& 82.85	& 50.87	& 67.74	\\
 similarities	& 77.24	& 91.07	& 67.76	& \bf 78.69	\\ 
  \hline
\hline  
 \bf en A (only)	& \good & \bad & \texttt{POT} & \texttt{F$_1$} \\\hline
 context		& 67.65	& 45.03	& 11.51		& 47.90	\\
 $n$-grams		& 71.22	& 40.12	& \,\,\,5.99	& 44.86	\\
 heuristics		& 76.46	& 41.94	& \,\,\,7.11	& 52.57	\\
 similarities		& 62.93	& 44.58	& \,\,\,9.62	& 46.16	\\
 \,\,\,\, lexical	& 62.25	& 41.46	& \,\,\,8.66	& 44.82	\\
 \,\,\,\, syntactic	& 59.18	& 36.20	& \,\,\,0.00	& 36.47	\\
 \,\,\,\, semantic	& 55.56	& 40.42	& \,\,\,9.92	& 42.16	\\
 \hline
 \hline
 \bf en A (without)	& \good & \bad	& \texttt{POT}	& \texttt{F$_1$} 
\\\hline
 context		& 76.05	& 41.53	& \,\,\,8.98	& 51.50	\\
 $n$-grams		& 77.25	& 45.56	& 12.23		& \bf 55.17\\
 heuristics		& 73.84	& 65.33	& \,\,\,6.81	& 48.66\\
 similarities		& 78.02	& 71.82	& \,\,\,9.88	& 53.24	\\
 \,\,\,\, lexical	& 78.23	& 72.81	& \,\,\,9.91	& 53.65	\\
 \,\,\,\, syntactic	& 78.81	& 43.89	& \,\,\,9.91	& 53.73	 \\
 \,\,\,\, semantic	& 78.41	& 71.82	& 10.30		& 53.51	 \\   
  \hline
  \hline
  \bf en B	& \yes & \no	& \texttt{UNS}	& \texttt{F$_1$}\\ \hline
 post$_1$	& 78.79	& 57.14	& 20.00		& 51.98 \\
 post$_2$	& 85.71	& 57.14	& 25.00 	& \bf 55.95 \\
 \hline
 \hline  
 \bf primary	& \texttt{D/G/Y}& \texttt{I/B/N}& \texttt{R/P/U} & 
\texttt{F$_1$}\\\hline
  ar		& 77.31		& 91.21		& 67.13	& 78.55 \\
  en A		& 78.45		& 72.39		& 10.40	& 53.74 \\
  en B 		& 80.00		& 44.44		& 36.36	& 53.60 \\
  \hline
 
 
 \end{tabular}
 \caption{Post-submission results for Arabic (ar) and English (en), for subtasks A and B.
 The lines marked with \emph{only} show results  using a particular type of features only,
 while those marked as \emph{without} show results
 when using all features but those of a particular type.
 The best results for each subtask are marked in bold;
 the results for our official primary submissions are included for comparison.
\label{tab:aftertask}}
\end{table}


% The first rows of Table~\ref{tab:aftertask} show the results obtained after 
% discarding each of the different features families.

% \begin{table}%[h]	
% \begin{tabular}{|l|crrr|}
% % \hline
% %   \multicolumn{5}{c}{Arabic} \\
% \hline  
%  Subm. without& \bf \texttt{DIR} & \bf \texttt{REL} & \bf \texttt{IREL} & 
% \bf \texttt{MACRO} \\\hline
%  $n$-grams	&	&	&	& 76.75	\\
%  cont$_1$	&	&	&	& 67.74	\\
%  no max iyas	&	&	&	& 79.13	\\
%  no max sim	&	&	&	& 78.37	\\
%  no max iyas sim&	&	&	& 78.69	\\ 
%   \hline
%  \end{tabular}
%  \caption{Post-competition experiments Arabic.\label{tab:aftertaskarabic}}
%  \end{table}

\subsection{English, Subtask A} \label{sec:discussiona}

We performed experiments similar to those we did for Arabic. According to the 
\textit{only} figures, the heuristic features seem to be the most useful ones, 
followed by the context-based ones. The latter explore a 
dimension ignored by the rest:  these features are completely 
uncorrelated and provide a good performance boost (as the \textit{without} 
experiments show). On the other hand, using all features but the $n$-grams 
improves over the performance of our primary run (F$_1=55.17$ 
compared to F$_1=53.74$). This is an interesting but not very significant 
result as these features had already boosted our performance
at development time. Further research is necessary.
  
\subsection{English, Subtask B} \label{sec:discussionb}

Our post-submission efforts focused on investigating why learning from 
the training data only was considerably better than learning from training+dev. The output 
labels on the test set in the two learning scenarios showed considerable 
differences: when learning from training+dev, the predicted labels were \yes for 
all but three cases. After correcting a bug in our implementation of the 
polarity-related features, the result when learning on training+dev became 
F$_1$=51.98 (Table~\ref{tab:aftertask}, post$_1$). Further 
analysis showed that the features counting the number of \good, \bad, and \pot 
comments within categories by the same user (\cf Section~\ref{sub:profile}) 
varied greatly when computed on training and on training+dev,
as the number of comments by a user in a category was, in most 
cases, too small to yield very reliable statistics. After discarding these three 
features, the F$_1$ raised to 55.95 (Table~\ref{tab:aftertask}, post$_2$),
which is higher than what we obtained at submission time.
Note that, once again, the \unsure class is by far the hardest to identify properly.

Surprisingly, learning with the bug-free implementation from the training set 
yielded a much higher F$_1$ of $69.35$ on the test dataset (not shown in the table).
Analysis revealed that the difference in performance was due to misclassifying 
just four questions. Indeed, the differences seem to 
occur due to the natural randomness of the classifier on a small test dataset and they 
cannot be considered statistically significant~\cite{Marquez-EtAl:2015:SemEval}. 

% \begin{table}%[h]	
% \begin{tabular}{|l|cccc|}
% % \hline
% %   \multicolumn{5}{c}{Subtask B} \\
% \hline  
% 	& \bf \yes & \bf \no & \bf \unsure & \bf \texttt{MACRO}	 \\
%  \hline
%  bug-free	& $78.79$	& $57.14$	& $20.00$	& $51.98$ \\
%  no-cat		& $85.71$	& $57.14$	& $25.00$ 	& $55.95$ \\
%  \hline
%  \end{tabular}
%  \caption{Post-competition experiments English B: \textit{bug-free} refers to 
% our run after correcting some implementation bugs; \textit{no-cat} is the 
% result after discarding the category-level per-user statistics.
% \label{tab:aftertaskb}}
%  \end{table}