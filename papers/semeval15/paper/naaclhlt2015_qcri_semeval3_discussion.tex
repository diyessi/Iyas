\section{Post-Submission Experiments}
\label{sec:discussion}

We carried out further experiments after the task deadline to understand how 
different feature families contributed to the performance of our classifiers. 
% The results on the corresponding test sets are reported in 
Table~\ref{tab:aftertask} reports the results on the different test sets. We 
managed to slightly raise the performance for the three tasks due to different 
reasons.

\subsection{Arabic} \label{sec:discussionArabic}

We ran experiments with the same framework as in the primary submission by 
considering both the different subsets of features in isolation (\textit{only}) 
or all the features except for a subset (\textit{without}). The $n$-grams 
features together with contrastive submission~1 allow for a slightly better 
performance than our already winning submission (F$_1=78.69$, compared to 
F$_1=78.55$). Our ranking approach (contrastive~1) represents the most 
important one to get such a good result. \blue{It is worth noting that the cont$_1$ submission is indeed based on similarity.}

\begin{table}[t]	
\footnotesize
\centering
% \begin{tabular}{|l|c@{\hskip 0.3cm}c@{\hskip 0.3cm}c@{\hskip 0.3cm}c|}
\begin{tabular}{|l|cccc|}
% \hline
%   \multicolumn{5}{c}{Subtask A} \\
\hline  
\bf ar (only)	& \texttt{DIR} & \texttt{IRREL} & \texttt{REL} & 
\texttt{F$_1$} 
\\\hline
 $n$-grams	& 30.40	& 41.07	& 72.27	& 47.91	\\
 cont$_1$	& 74.89	& 63.68	& 91.23	& 76.60	\\
 similarities	& 61.83	& 25.63	& 82.55	& 56.67	\\
%  \bf ar (only)	& \texttt{DIR} & \texttt{REL} & \texttt{IRREL} & \texttt{F$_1$} 
% \\\hline
%  $n$-grams	& 30.40	& 72.27	& 41.07	& 47.91	\\
%  cont$_1$	& 74.89	& 91.23	& 63.68	& 76.60	\\
%  similarities	& 61.83	& 82.55	& 25.63	& 56.67	\\ 
  \hline
  \hline  
 \bf ar (without)& \texttt{DIR} & \texttt{REL} & \texttt{IRREL} &\texttt{F$_1$} 
\\\hline
 $n$-grams	& 75.51	& 91.31	& 63.85	& 76.89	\\
 cont$_1$	& 69.50	& 82.85	& 50.87	& 67.74	\\
 similarities	& 77.24	& 91.07	& 67.76	& \bf 78.69	\\ 
  \hline
\hline  
 \bf en A (only)	& \good & \bad & \texttt{POT} & \texttt{F$_1$} \\\hline
 context		& 67.65	& 45.03	& 11.51		& 47.90	\\
 $n$-grams		& 71.22	& 40.12	& \,\,\,5.99	& 44.86	\\
 heuristics		& 76.46	& 41.94	& \,\,\,7.11	& 52.57	\\
 similarities		& 62.93	& 44.58	& \,\,\,9.62	& 46.16	\\
 \,\,\,\, lexical	& 62.25	& 41.46	& \,\,\,8.66	& 44.82	\\
 \,\,\,\, syntactic	& 59.18	& 36.20	& \,\,\,0.00	& 36.47	\\
 \,\,\,\, semantic	& 55.56	& 40.42	& \,\,\,9.92	& 42.16	\\
 \hline
 \hline
 \bf en A (without)	& \good & \bad	& \texttt{POT}	& \texttt{F$_1$} 
\\\hline
 context		& 76.05	& 41.53	& \,\,\,8.98	& 51.50	\\
 $n$-grams		& 77.25	& 45.56	& 12.23		& \bf 55.17\\
 heuristics		& 73.84	& 65.33	& \,\,\,6.81	& 48.66\\
 similarities		& 78.02	& 71.82	& \,\,\,9.88	& 53.24	\\
 \,\,\,\, lexical	& 78.23	& 72.81	& \,\,\,9.91	& 53.65	\\
 \,\,\,\, syntactic	& 78.81	& 43.89	& \,\,\,9.91	& 53.73	 \\
 \,\,\,\, semantic	& 78.41	& 71.82	& 10.30		& 53.51	 \\   
  \hline
  \hline
  \bf en B	& \yes & \no	& \texttt{UNS}	& \texttt{F$_1$}\\ \hline
 post$_1$	& 78.79	& 57.14	& 20.00		& 51.98 \\
 post$_2$	& 85.71	& 57.14	& 25.00 	& \bf 55.95 \\
 \hline
 \hline  
 \bf primaries	& \texttt{D/G/Y}& \texttt{I/B/N}& \texttt{R/P/U} & 
\texttt{F$_1$}\\\hline
  ar		& 77.31		& 91.21		& 67.13	& 78.55 \\
  en A		& 78.45		& 72.39		& 10.40	& 53.74 \\
  en B 		& 80.00		& 44.44		& 36.36	& 53.60 \\
  \hline
 
 
 \end{tabular}
 \caption{Post-competition results for Arabic (ar) and English (en) A and B 
tasks. Best results per task highlighted. Primary submissions included for 
comparison.
\label{tab:aftertask}}
\end{table}






% The first rows of Table~\ref{tab:aftertask} show the results obtained after 
% discarding each of the different features families.

% \begin{table}%[h]	
% \begin{tabular}{|l|crrr|}
% % \hline
% %   \multicolumn{5}{c}{Arabic} \\
% \hline  
%  Subm. without& \bf \texttt{DIR} & \bf \texttt{REL} & \bf \texttt{IREL} & 
% \bf \texttt{MACRO} \\\hline
%  $n$-grams	&	&	&	& 76.75	\\
%  cont$_1$	&	&	&	& 67.74	\\
%  no max iyas	&	&	&	& 79.13	\\
%  no max sim	&	&	&	& 78.37	\\
%  no max iyas sim&	&	&	& 78.69	\\ 
%   \hline
%  \end{tabular}
%  \caption{Post-competition experiments Arabic.\label{tab:aftertaskarabic}}
%  \end{table}

\subsection{English Task A} \label{sec:discussiona}

We performed similar experiments as the ones for Arabic. According to the 
\textit{(only)} figures, the heuristic features seem to be the most useful ones, 
followed by the context-based information. The latter features explore a 
dimension completely ignored by other features:  they are completely 
uncorrelated and provide a good performance boosting (as the \textit{without} 
experiment shows). On the other side, using all the features but the $n$-grams 
allows for a better performance than that in the primary run (F$_1=55.17$ 
compared to F$_1=53.74$) . This is an interesting but not very significant 
result as these features had significantly pushed up the performance of our 
system at development time. Further research is necessary.
  
\subsection{English Task B} \label{sec:discussionb}

Our post-task efforts are intended to investigate on the reasons why learning on 
training only was considerably better than learning on training+dev. Output 
labels on the test set in the two learning scenarios showed considerable 
differences: when learning on training+dev, the predicted labels were \yes on 
all but three cases. After correcting a bug in our implementation of the 
polarity-related features, the result obtained by learning on training+dev was 
F$_1=51.98$ (Table~\ref{tab:aftertask}, post$_1$). Further feature-based 
analyses showed that the features counting the number of \good, \bad, and \pot 
comments within categories from the same user (\cf Section~\ref{sub:profile}) 
varied greatly when computed on the training or training+dev datasets. The 
reason is that the number of comments from a user in a category is, in most 
cases, too limited to generate reliable statistics. After discarding these three 
features, the F$_1$ raised to $55.95$ (Table~\ref{tab:aftertask}, post$_2$). 
This figures represent a higher performance than that obtained at submission 
time. Observe that, once again, the \unsure class is the hardest to identify 
properly.

Surprisingly, applying the bug-free implementation on the training set only 
still allowed for a higher F$_1 =69.35$ on test. A manual analysis allowed us 
to observe that the difference in performance was the result of misclassifying 
only four questions either as \yes or \unsure. Indeed, the differences seem to 
occur due to the randomness of the classifier on a small dataset and they 
cannot be considered statistically 
significant~\cite{Marquez-EtAl:2015:SemEval}. 

% \begin{table}%[h]	
% \begin{tabular}{|l|cccc|}
% % \hline
% %   \multicolumn{5}{c}{Subtask B} \\
% \hline  
% 	& \bf \yes & \bf \no & \bf \unsure & \bf \texttt{MACRO}	 \\
%  \hline
%  bug-free	& $78.79$	& $57.14$	& $20.00$	& $51.98$ \\
%  no-cat		& $85.71$	& $57.14$	& $25.00$ 	& $55.95$ \\
%  \hline
%  \end{tabular}
%  \caption{Post-competition experiments English B: \textit{bug-free} refers to 
% our run after correcting some implementation bugs; \textit{no-cat} is the 
% result after discarding the category-level per-user statistics.
% \label{tab:aftertaskb}}
%  \end{table}