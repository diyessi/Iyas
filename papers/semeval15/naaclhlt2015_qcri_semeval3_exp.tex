\section{Submissions and Results}
\label{sec:experiments}

Following we describe our three primary submissions to the three subtasks. The 
contrastive submissions (two per task) are discussed later on. 
Table~\ref{tab:results} includes the results obtained with these submissions.

% \blue{Massimo/Simone: please, give more details about this, if necessary}


\begin{table}%[h]
 \begin{tabular}{|l|c@{\hskip 0.2cm}c@{\hskip 0.2cm}c@{\hskip 0.2cm}c|}
%   \multicolumn{5}{c}{Subtask A} \\
%   \hline
%  Subm.		& \bf \good & \bf\bad 	& \bf \texttt{POT} & 	
%  \\
  \hline
  \bf ar& \bf\dir & \bf\rel &\bf \texttt{IRREL} & \bf\texttt{MACRO}\\  \hline  
  prim	& $77.31$ & $91.21$	& $67.13$	& $78.55$ \\
  cont$_1$	& $74.89$ & $91.23$	& $63.68$	& $76.60$ \\
  cont$_2$	& $76.63$ & $90.30$	& $63.98$	& $76.97$ \\
    \hline \hline
 

  \bf en A & \bf \good & \bf\bad & \bf \texttt{POT} & \bf\texttt{MACRO} \\\hline
  prim  & $78.45$	& $72.39$	& $10.40$	& $53.74$ \\
  cont$_1$ & $76.08$	& $75.68$	& $17.44$	& $56.40$ \\
  cont$_2$ & $75.46$	& $72.48$ 	& $7.97$	& $51.97$ \\
% \end{tabular}
% \begin{tabular}{|l|crrr|}
% \hline
%   \multicolumn{5}{c}{Subtask B} \\
\hline  \hline
 \bf en B	& \bf \yes & \bf \no & \bf \unsure & \bf \texttt{MACRO}	 \\
  \hline
  
  prim	& $80.00$	& $44.44$	& $36.36$	& $53.60$ \\
  cont$_1$& $75.68$	& $0.00$	& $0.00$ & $25.23$ \\
  cont$_2$ & $66.67$	& $33.33$ 	& $47.06$	& $49.02$ \\
  \hline
 \end{tabular}
\caption{Per-class and overall $F_1$-measure of our \textit{prim}ary and 
\textit{cont}rastive submissions to SemEval Task 3 for Arabic (A) and English 
(en).
% The columns 2-4 specify the F1 measure of 
% the binary learning problem, for example Good VS all other classes. Finally 
the 
% 6-th column shows the official F1 score for the submission.
\label{tab:results}}
\end{table}

\subsection{Primary Submissions}

In general, our approaches perform multiclass classification on the basis of a 
one-vs-rest support vector machines strategy (\ie we train one classifier for 
each class). Our classifications for both Arabic and English A are made at 
comment level.


\paragraph{Arabic} Our submission is based on a logistic regression approach. 
The utilized features are lexical similarities (Section~\ref{sub:sim}) and  
$n$-Grams (Section~\ref{ssub:ngrams}), together with the predictions obtained 
with our contrastive submission~1 (\cf Section~\ref{sub:contrastive}). 

\paragraph{English A}
The submission applies the linear-kernel SVM for model estimation from 
scikit-learn.%
\footnote{http://scikit-learn.org/stable/}
We used a one-versus-all approach to account for the fact that the learning 
problem is a multiclass one. We tuned the $C$ hyper-parameter of the SVM in 
order to deal with the class imbalance ---by increasing the value of $C$, we 
built more complex classifiers for those classes with less instances. 

The features for this submission consist of lexical and semantic similarities 
(Section~\ref{sub:sim}), context information (Section~\ref{ssub:context}), 
$n$-Grams (Section~\ref{ssub:ngrams}), and heuristics 
(Section~\ref{ssub:heuristics}). In a sort of stacking, the output of our 
rule-based system from the contrastive submission~2 is included as another 
feature. 

\paragraph{English B}

Following the strategy applied during the manual labelling of the \yes/\no 
questions by the task organizers~\cite{Marquez-EtAl:2015:SemEval}, our approach 
to task B is divided in three steps:
\Ni identifying the \good comments among those associated to the question;
\Nii classifying each of the \good comments as \yes, \no, or \unsure; and 
\Niii aggregation. 
The overall answer to a question is that of the majority of the comments. In 
case of draw, we opt for labeling it as \unsure.%
\footnote{\yes, the majority class in the training and development partitions, 
could have been the default answer. Still, we opted for a conservative decision: 
deciding \unsure if no evidence enough was at hand.}
Step \Ni is indeed task A. As for step \Nii, our approach to this task is 
identical as that for English A, but adding the features described in 
Sections~\ref{sub:polarity} and~\ref{sub:profile}.

Differently to the rest of 
submissions, our submitted results were obtained with a classifier trained on 
the training data only (the development set was neglected). The reason behind 
this decision was obtaining an unexpected distribution of (mostly) \yes answers 
on the test set (completely different to that observed in both training 
and development partitions. Further experiments carried out after the 
submission demonstrated that the causes for such an unexpected behavior were a 
few features for which no statistics enough existed to be valuable as well as a
buggy implementation of some of the other features. Further discussion is 
included in Section~\ref{sec:discussionb}.


\subsection{Contrastive Submissions}
\label{sub:contrastive}

\paragraph{Arabic} 

Our contrastive submission~1 consists of a ranking problem. The similarity 
$sim(q,c)$ for every $c$ in the question is computed, after stopwording and 
stemming, as 
\begin{equation}
 sim(q,c) = \frac{1}{|q|} \sum_{t\in q\cup c} \omega(t) \enspace ,
 \label{eq:overlap}
\end{equation}
% 
where $\omega(t)$ is the empirically-set weight of an $n$-gram: $\omega = 1$ 
for $1$-grams and $\omega = 4$ for $2$-grams. Given the 5 comments 
$c_1,\ldots,c5\in C$ associated to $q$, the maximum similarity $\max_C sim(q,c)$ 
is mapped to a maximum 100\% similarity and the rest of the scores are mapped 
proportionally. Each comment is assigned a class according to the following 
ranges: [80, 100]\% for \dir, (20,80)\% for \rel, and [0,20]\% for \irel.


As for the contrastive submission~2, we built a binary classifier based on 
logistic regression: \dir or no. The comments are then sorted according to the 
classifier's prediction confidence and the final labels are assigned 
accordingly: \dir for the 1st ranked, \rel for the 2nd ranked, and \irel for the 
rest. Only lexical similarities are included as features (discarding those 
weighted with idf variants).

\paragraph{English A}

For our contrastive submission~1, the same machine learning schema as 
for the primary submission is used. In contrast to the primary submission, 
this time the $C$ hyper-parameter is set to the default value and the class 
imbalance was dealt by tuning the $j$ parameter (cost of making mistakes on 
positive examples). In this case we use SVM$^\mathrm{light}$~\cite{Joachims:99}.

Our English contrastive submission~2 operates in the same way as the Arabic 
contrastive submission~1. The applied ranges are the same, this time used to 
assign the classes \good, \pot, and \bad. Some heuristics override the so 
generated decisions: $c$ is classified as \good if it includes a URL, starts 
with an imperative verb (\eg \textit{try}, \textit{view}, \textit{contact}, 
\textit{check}), or contains \textit{yes words} (\eg \textit{yes}, \textit{yep}, 
\textit{yup}) or \textit{no words} (\eg \textit{no}, \textit{nooo}, 
\textit{nope}). Comments written by the author of the question or including 
acknowledgments are considered \dial; which become \bad comments. 
%   only rule-based features 
% (Section~\ref{ssub:heuristics}) \blue{confirm}


\paragraph{English B}

Our contrastive submission~1 is identical as the primary one, but considering 
both training and development data for estimating the model.
 
 
The contrastive submission~2 consists of a rule-based system. A comment is 
labeled as \yes if it starts with affirmative words: \textit{yes}, \textit{yep}, 
\textit{yeah}, etc.
% \footnote{\blue{complete it or cite the source for affirmative words; the same 
% for the rest}}
It is labeled as \no if it starts with \textit{no}, \textit{nop}, 
\textit{nope}, etc‚Äù, 




%  \begin{table}[h]
%  \begin{tabular}{c|c|c|c|c}
%   \multicolumn{5}{c}{Subtask A} \\
%   \hline
%  Subm.		& F1-Good 	& F1-Bad 	& F1-Pot 	& F1	 \\
%   \hline
%   E. P.  & $78.45$	& $72.39$	& $10.40$	& $53.74$ \\
%   E. C1 & $76.08$	& $75.68$	& $17.44$	& $56.40$ \\
%   E. C2 & $75.46$	& $72.48$ 	& $7.97$	& $51.97$ \\
%   A. P. & $77.31$	& $91.21$	& $67.13$	& $78.55$ \\
%   A. C1& $74.89$ 	& $91.23$	& $63.68$	& $76.60$ \\
%   A. C2& $76.63$	& $90.30$	& $63.98$	& $76.97$ \\
% \hline 
%   \multicolumn{5}{c}{Subtask B} \\
% \hline  
%  Subm.		& F1-Yes 	& F1-No 	& F1-Unsure 	& F1	 \\
%   \hline
%   E. P.	& $78.45$	& $72.39$	& $10.40$	& $53.74$ \\
%   E. C1  & $76.08$	& $75.68$	& $17.44$	& $56.40$ \\
%   E. C2 & $75.46$	& $72.48$ 	& $7.97$	& $51.97$ \\
%  \end{tabular}
% \caption{Results of our experiments on SemEval Task 3. The first column specify 
% the submission type, English (E) or Arabic (A) and Primary (P) or Contrastive 1 
% or 2 (C1 or C2). The columns 2-4 specify the F1 measure of the binary learning 
% problem, for example Good VS all other classes. Finally the 6-th column shows 
% the official F1 score for the submission.\label{tab:results}}
% \end{table}



% - Clearly define tuning, submission and post-submission
% 
% Potentially interesting experiments to show/discuss
% 
% \textbf{English A}
% \begin{itemize}
%  \item Feature sets in isolation. 
%  \item Altogether
%  \item Simplified experiment: Potential becomes bad; Good vs Bad
%  \item Error analysis: Potential versus Good/Bad errors
% \end{itemize}

\subsection{Discussion and Further Experiments} \label{sec:expdiscussion}


% \begin{table*}
% \begin{tabular}{|c|c|c|c|c|c|c|c|}
% \hline
% Exp&Lexical & Syntactic & Semantic & Context & $n$-grams & Heuristics\\ 
% \hline
% 1	& x	& 	& 	& 	& 	& 		\\
% 2	& 	& x	& 	& 	& 	& 		\\
% 3	& 	& 	& x	& 	& 	& 		\\
% 4	& 	& 	& 	& x	& 	& 		\\
% 5	& 	& 	& 	& 	& x	& 		\\
% 6	& 	& 	& 	& 	& 	& x		\\
% 7	& x	& x	& x	& 	& 	& 	 	\\
% \hline
% \end{tabular}
% 
% 
% 
% \begin{tabular}{|l|c|c|c|c|c|c|c|}
% \hline
% Exp	&Lexical & Syntactic & Semantic & Context & $n$-grams & Heuristics\\ 
% \hline
% Massimo	& x	& x	& 	& 	& x	& 		\\
% % Iman	& 	& 	& 	& 	& 	& 		\\
% Hamdy	& 	& 	& 	& 	& 	& x		\\
% Simone	& 	& 	& x	& x	& 	& x		\\
% Wei	& x	& 	& 	& 	& 	& 		\\
% Preslav	& 	& 	& x	& 	& 	& 		\\
% 
% \hline
% \end{tabular}


% 
% \caption{Experiments with features subsets.} 
%  
%  
%  
% \end{table*}




\textbf{English B}
\begin{itemize}
 \item Any proposal? Probably no questions enough for an error analysis
\end{itemize}



\textbf{Arabic}

\begin{itemize}
 \item Let's ask the Arabic team!
\end{itemize}

% these are the submissions for English Subtask A and the Arabic Task
% 
% English Subtask A
% - subtask_a_python_primary.pred (Python system including ngrams and features from everybody)
% - subtask_a_svmlight_contrastive1.pred (Python system feature vectors, learning and classification carried out with svmlight and tuned parameters)
% - subtask_a_hamdy_contrastive2.pred (predictions by Hamdy's system)
% 
% Arabic Task
% - arabic_max_hamdy_primary.pred (Python system including Hamdy's predictions as features)
% - arabic_hamdy_contrastive1.pred (predictions by Hamdy's system)
% - arabic_max_contrastive2.pred (Python system with no ngrams and Hamdy rule + Hamdy's predictions as features)
% 
% Since the primary system for Arabic without Hamdy's predictions was weaker than the second contrastive on the dev set, I used the latter. It's the system using only similarity features (no ngrams), classifier parameters crossvalidated on the train set and Hamdy's rule for selecting Direct, Relevant and Irrelevant comments.

%SUBTASK B
% we just defined the three submissions, which you can find attached:
% 
% a) subtask_a_primary.pred was generated by our standard pipeline for
% task B, trained on the Iman's features for the training partition *only*
% b) subtask_a_contrastive1.pred is as the primary one, but using the
% train+dev features set generated by Iman
% c) subtask_a_contrastive2.pred includes Hamdy's predictions.
% 
% Regarding submission a, remember we were trying three "comment
% filtering" alternatives to select the adequate evidence when deciding
% the label Yes, No or Unsure on the entire question. Whereas on the dev
% set we observed that these three strategies impacted on the final
% result, such a behaviour disappeared on the test, so the three
% alternatives became 1.
% 
% Beside that, we found a somehow unexpected behaviour when classifying
% the test set with our system trained on the new train+dev set generated
% by Iman. In brief, almost 90% of the questions were classified as "Yes";
% a distribution unexpected with respect to that from the development set.
% As a result, this output became our contrastive submission 2
% 
% Regarding (c), I do not have information about Hamdy's prediction at hand.
