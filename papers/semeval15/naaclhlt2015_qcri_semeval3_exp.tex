\section{Experiments and Results}
\label{sec:experiments}

We made three submissions for each of the proposed subtasks: one primary and 
two contrastive. 

The submissions for the Arabic task use the following features:

\begin{description}
\item[Primary:] 
  lexical similarities (Section~\ref{sub:sim}) and  $n$-Grams 
  (Section~\ref{ssub:ngrams}). The predictions from the contrastive 
  submission~1 are included as yet another feature. 
\item[Contrastive 1:] 
%   the features from the primary submission plus the syntactic similarity ones 
%   (Section~\ref{ssub:sim}); and
  Predictions based on heuristics and the similarity as computed in 
  Eq.~\eqref{eq:overlap}. \blue{further descriptions in Hamdy contrastive 
  subsection}    
\item[Contrastive 2:] 
  The features include the lexical similarities together with the predictions 
  from contrastive 1. A classifier is estimated considering two classes only: 
  \dir or no-\dir. The comments are sorted according to the classifier's 
  prediction confidence and the final labels are assigned accordingly: \dir for 
  the 1st rakned, \rel for the 2nd, and \irel for the rest.
  
  
  only rule-based features 
(Section~\ref{ssub:heuristics}) \blue{confirm}
%  contextual (Section~\ref{ssub:context}); 
%  \item n-Grams (Section~\ref{ssub:ngrams});
%  \item Heuristics (Section~\ref{ssub:heuristics});
\end{description}

Primary and contrastive~2 submissions are based on a logistic regression. 
\blue{more details required}



The submissions for Subtask A-English make use of the following features: 
\begin{description}
\item[Primary:] 
  lexical similarity, semantic similarity (Section~\ref{sub:sim}), contextual 
  (Section~\ref{ssub:context}), $n$-Grams (Section~\ref{ssub:ngrams}), and
  Heuristics (Section~\ref{ssub:heuristics}). Once again, the predictions from 
  contrastive 2 are included as one more feature. 
\item[Contrastive 1:] 
%   the features from the primary submission plus the syntactic similarity ones 
%   (Section~\ref{ssub:sim}); and
  Same as submission 1, but it uses SVMLight to carry out the learning. Thus 
  this version exploit the j parameter for tuning the cost of mistakes for 
  positive classes.
    
\item[Contrastive 2:] 
  only rule-based features 
(Section~\ref{ssub:heuristics}) \blue{confirm}
%  contextual (Section~\ref{ssub:context}); 
%  \item n-Grams (Section~\ref{ssub:ngrams});
%  \item Heuristics (Section~\ref{ssub:heuristics});
\end{description}
% {\it i)} lexical similarity, semantic similarity (Sections~\ref{ssub:sim}); 
% {\it ii)} contextual (Section~\ref{ssub:context}); 
% {\it iii) n-Grams (Section~\ref{ssub:ngrams})};
% {\it iv) Heuristics (Section~\ref{ssub:heuristics})};
% 
Both primary and contrastive 1 submissions use a linear-kernel SVM for model 
estimation~\cite{}. The one-versus-all approach has been used to account for the fact 
that the learning problem is a multiclass one. The $C$ hyperparameter of the SVM 
was used to deal with the class imbalance for the primary submission: more complex classifiers were built for those classes with less instances, increasing the value of C. The $C$ hyper-parameter is set to the default value for the contrastive 1 submission, while the class imbalance was dealt with tuning the j parameter (cost of making mistakes on positive examples) of a different tool used for carrying out learning, namely SVMLight. Its parameters 
were selected on the development set. Since the contrastive 2 submission is based on rules, no learning procedure was required (Section~\ref{ssub:heuristics}).

Finally, the submissions for English Subtask B make use of the following 
features:
\begin{description}
 \item[Primary:] the same ones of the Subtask A-English plus the features 
described in Section~\ref{sub:app_enB}, and the model is selected on the 
training set;\blue{we have to say more about this issue} 
 \item[Contrastive 1:] as the primary submission, but by considering both 
training and development data for estimating the model; and
 \item[Contrastive 2:] a rule-based system as described in 
Section~\ref{tmp}.
 \end{description}

%  \begin{table}[h]
%  \begin{tabular}{c|c|c|c|c}
%   \multicolumn{5}{c}{Subtask A} \\
%   \hline
%  Subm.		& F1-Good 	& F1-Bad 	& F1-Pot 	& F1	 \\
%   \hline
%   E. P.  & $78.45$	& $72.39$	& $10.40$	& $53.74$ \\
%   E. C1 & $76.08$	& $75.68$	& $17.44$	& $56.40$ \\
%   E. C2 & $75.46$	& $72.48$ 	& $7.97$	& $51.97$ \\
%   A. P. & $77.31$	& $91.21$	& $67.13$	& $78.55$ \\
%   A. C1& $74.89$ 	& $91.23$	& $63.68$	& $76.60$ \\
%   A. C2& $76.63$	& $90.30$	& $63.98$	& $76.97$ \\
% \hline 
%   \multicolumn{5}{c}{Subtask B} \\
% \hline  
%  Subm.		& F1-Yes 	& F1-No 	& F1-Unsure 	& F1	 \\
%   \hline
%   E. P.	& $78.45$	& $72.39$	& $10.40$	& $53.74$ \\
%   E. C1  & $76.08$	& $75.68$	& $17.44$	& $56.40$ \\
%   E. C2 & $75.46$	& $72.48$ 	& $7.97$	& $51.97$ \\
%  \end{tabular}
% \caption{Results of our experiments on SemEval Task 3. The first column specify 
% the submission type, English (E) or Arabic (A) and Primary (P) or Contrastive 1 
% or 2 (C1 or C2). The columns 2-4 specify the F1 measure of the binary learning 
% problem, for example Good VS all other classes. Finally the 6-th column shows 
% the official F1 score for the submission.\label{tab:results}}
% \end{table}

  \begin{table*}%[h]
 \begin{tabular}{|l|cccc|}
  \multicolumn{5}{c}{Subtask A} \\
%   \hline
%  Subm.		& \bf \good & \bf\bad 	& \bf \texttt{POT} & 	
%  \\
  \hline
  Arabic& \bf\dir & \bf\rel &\bf \irel & \bf\texttt{MACRO}\\  
  \,\,\,\,prim	& $77.31$ & $91.21$	& $67.13$	& $78.55$ \\
  \,\,\,\,cont$_1$	& $74.89$ & $91.23$	& $63.68$	& $76.60$ \\
  \,\,\,\,cont$_2$	& $76.63$ & $90.30$	& $63.98$	& $76.97$ \\
  \hline
  English 	& \bf \good & \bf\bad 	& \bf \texttt{POT} & \bf\texttt{MACRO}
\\
  \,\,\,\,prim  & $78.45$	& $72.39$	& $10.40$	& $53.74$ \\
  \,\,\,\,cont$_1$ & $76.08$	& $75.68$	& $17.44$	& $56.40$ \\
  \,\,\,\,cont$_2$ & $75.46$	& $72.48$ 	& $7.97$	& $51.97$ \\
\hline 
\end{tabular}
\begin{tabular}{|l|crrr|}
% \hline
  \multicolumn{5}{c}{Subtask B} \\
\hline  
 Subm.		& \bf \yes & \bf \no & \bf \unsure & \bf \texttt{MACRO}	 \\
  \hline
  
  \,\,prim	& $80.00$	& $44.44$	& $36.36$	& $53.60$ \\
  \,\,cont$_1$& $75.68$	& $0.00$	& $0.00$ & $25.23$ \\
  \,\,cont$_2$ & $66.67$	& $33.33$ 	& $47.06$	& $49.02$ \\
  \hline
 \end{tabular}
\caption{$F$-measure of our experiments on SemEval Task 3. The first 
column specify the submission type, English (E) or Arabic (A) and Primary (P) 
or Contrastive 1 or 2 (C1 or C2). The columns 2-4 specify the F1 measure of the 
binary learning problem, for example Good VS all other classes. Finally the 
6-th column shows the official F1 score for the submission.\label{tab:results}}
\end{table*}

% - Clearly define tuning, submission and post-submission
% 
% Potentially interesting experiments to show/discuss
% 
% \textbf{English A}
% \begin{itemize}
%  \item Feature sets in isolation. 
%  \item Altogether
%  \item Simplified experiment: Potential becomes bad; Good vs Bad
%  \item Error analysis: Potential versus Good/Bad errors
% \end{itemize}

\subsection{Discussion and Further Experiments} \label{sec:expdiscussion}


\begin{table*}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Exp&Lexical & Syntactic & Semantic & Context & $n$-grams & Heuristics\\ 
\hline
1	& x	& 	& 	& 	& 	& 		\\
2	& 	& x	& 	& 	& 	& 		\\
3	& 	& 	& x	& 	& 	& 		\\
4	& 	& 	& 	& x	& 	& 		\\
5	& 	& 	& 	& 	& x	& 		\\
6	& 	& 	& 	& 	& 	& x		\\
7	& x	& x	& x	& 	& 	& 	 	\\
\hline
\end{tabular}



\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
Exp	&Lexical & Syntactic & Semantic & Context & $n$-grams & Heuristics\\ 
\hline
Massimo	& x	& x	& 	& 	& x	& 		\\
% Iman	& 	& 	& 	& 	& 	& 		\\
Hamdy	& 	& 	& 	& 	& 	& x		\\
Simone	& 	& 	& x	& x	& 	& x		\\
Wei	& x	& 	& 	& 	& 	& 		\\
Preslav	& 	& 	& x	& 	& 	& 		\\

\hline
\end{tabular}



\caption{Experiments with features subsets.} 
 
 
 
\end{table*}



\subsection{Contrastive Hamdy}
\label{tmp}

We approach the Arabic task as a ranking problem, Given the 5 comments 
associated to a question, the most similar one is assigned the maximum 100\% 
similarity and the rest of the scores are mapped proportionally. The ranges 
for the three classes are [80, 100] for \dir, (20,80) for \rel, and [0,20] 
for \irel.

For English, the ranges are \blue{equivalent} for \good, \pot, and \bad 
comments. Additionally, some heuristics override the so generated decisions: a 
$c$ is labeled as \good if
\Ni it contains a URL or
\Nii it starts with an affirmation (and the question is of type YES/NO), and as 
\bad if $c$ is written by $u_q$ or contains an acknowledgement.

\textbf{English B}
\begin{itemize}
 \item Any proposal? Probably no questions enough for an error analysis
\end{itemize}



\textbf{Arabic}

\begin{itemize}
 \item Let's ask the Arabic team!
\end{itemize}

% these are the submissions for English Subtask A and the Arabic Task
% 
% English Subtask A
% - subtask_a_python_primary.pred (Python system including ngrams and features from everybody)
% - subtask_a_svmlight_contrastive1.pred (Python system feature vectors, learning and classification carried out with svmlight and tuned parameters)
% - subtask_a_hamdy_contrastive2.pred (predictions by Hamdy's system)
% 
% Arabic Task
% - arabic_max_hamdy_primary.pred (Python system including Hamdy's predictions as features)
% - arabic_hamdy_contrastive1.pred (predictions by Hamdy's system)
% - arabic_max_contrastive2.pred (Python system with no ngrams and Hamdy rule + Hamdy's predictions as features)
% 
% Since the primary system for Arabic without Hamdy's predictions was weaker than the second contrastive on the dev set, I used the latter. It's the system using only similarity features (no ngrams), classifier parameters crossvalidated on the train set and Hamdy's rule for selecting Direct, Relevant and Irrelevant comments.

%SUBTASK B
% we just defined the three submissions, which you can find attached:
% 
% a) subtask_a_primary.pred was generated by our standard pipeline for
% task B, trained on the Iman's features for the training partition *only*
% b) subtask_a_contrastive1.pred is as the primary one, but using the
% train+dev features set generated by Iman
% c) subtask_a_contrastive2.pred includes Hamdy's predictions.
% 
% Regarding submission a, remember we were trying three "comment
% filtering" alternatives to select the adequate evidence when deciding
% the label Yes, No or Unsure on the entire question. Whereas on the dev
% set we observed that these three strategies impacted on the final
% result, such a behaviour disappeared on the test, so the three
% alternatives became 1.
% 
% Beside that, we found a somehow unexpected behaviour when classifying
% the test set with our system trained on the new train+dev set generated
% by Iman. In brief, almost 90% of the questions were classified as "Yes";
% a distribution unexpected with respect to that from the development set.
% As a result, this output became our contrastive submission 2
% 
% Regarding (c), I do not have information about Hamdy's prediction at hand.
